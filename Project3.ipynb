{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_ubyte\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import  flood_fill\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.feature import hog\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import rectangle\n",
    "import math\n",
    "from matplotlib.pyplot import bar\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import hough_line, hough_line_peaks, rotate\n",
    "import joblib\n",
    "import time\n",
    "import glob\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_crop(binary_image: np.ndarray) -> np.ndarray:\n",
    "    all_points = cv2.findNonZero(binary_image)\n",
    "    x, y, w, h = cv2.boundingRect(all_points)\n",
    "    height, width = binary_image.shape\n",
    "    border = 0\n",
    "    left = max(0, x - border)\n",
    "    right = min(width, x + w + border)\n",
    "    top = max(0, y - border)\n",
    "    bottom = min(height, y + h + border)\n",
    "    return binary_image[top:bottom, left:right], top, bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## return hog features from input image #########\n",
    "def extract_hog_features(img):\n",
    "    img = cv2.resize(img, (110, 200))\n",
    "    fd = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualize=False)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## binarize image ##########\n",
    "def local_binarize(img, block_size = 35, offset_val = 10):\n",
    "    img=img_as_ubyte(img)\n",
    "    b_img= img < threshold_otsu(img)\n",
    "    return b_img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### function that returns the row of the text basline #######\n",
    "def baseline(img):\n",
    "    horizontal_projection = np.sum(img, axis=1)\n",
    "    return np.argmax(horizontal_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### function that takes the folder of the dataset then returns array of images and their labels ########\n",
    "def load_images_from_folder(folders):\n",
    "    images = []\n",
    "    y=[]\n",
    "    i = 1\n",
    "    for folder in folders:   \n",
    "        for filename in os.listdir(folder):\n",
    "            img = io.imread(os.path.join(folder,filename), 1)\n",
    "            if img is not None:\n",
    "                b_img=local_binarize(img)\n",
    "                baseline_i=baseline(b_img)\n",
    "                if np.sum(b_img[baseline_i])>=(b_img.shape[1]-2):\n",
    "                    b_img= 1-b_img\n",
    "                images.append(b_img)\n",
    "                \n",
    "                y.append(i)\n",
    "        i+=1\n",
    "    return images, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Horizontal profile projection ###########\n",
    "def HPP(img):\n",
    "    horizontal_projection = np.sum(img, axis=1)\n",
    "    return np.histogram(horizontal_projection, bins=10)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contour(img,minArea):\n",
    "    find_contour_img=find_contours(img, 0.8)\n",
    "    i=0\n",
    "    vertical_lines_height = []\n",
    "    max_height = 0.0001\n",
    "    variance=0\n",
    "    for box in find_contour_img:  \n",
    "        Xmin=min(box[:,1])\n",
    "        Xmax=max(box[:,1])\n",
    "        Ymin=min(box[:,0])\n",
    "        Ymax=max(box[:,0])\n",
    "        if (Ymax-Ymin)*(Xmax-Xmin)>minArea:\n",
    "            vertical_lines_height.append(Ymax-Ymin)\n",
    "            i+=1\n",
    "    if vertical_lines_height != []:\n",
    "        max_height=max(vertical_lines_height)\n",
    "        variance=np.var(vertical_lines_height)\n",
    "    return i, max_height, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contours_full(img,minArea):\n",
    "    img=img.astype('uint8')\n",
    "    find_contour_img=find_contours(img,0.8)\n",
    "    final= np.zeros(img.shape)\n",
    "    i=0\n",
    "    a=[]\n",
    "    for box in find_contour_img:  \n",
    "        Xmin=min(box[:,1])\n",
    "        Xmax=max(box[:,1])\n",
    "        Ymin=min(box[:,0])\n",
    "        Ymax=max(box[:,0])\n",
    "        if (Ymax-Ymin)*(Xmax-Xmin)>minArea:\n",
    "            rr, cc = rectangle(start = (math.ceil(0),math.ceil(Xmin)), end = (math.ceil(img.shape[0]),math.ceil(Xmax)), shape=img.shape)\n",
    "            final[rr,cc]=1\n",
    "            i+=1\n",
    "            a.append([[Ymax,Xmax],[Ymax,Xmin],[Ymin,Xmin],[Ymin,Xmax]])\n",
    "\n",
    "    return i, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_white_ratio(b_img):\n",
    "    white=np.sum(b_img==1)\n",
    "    black=np.sum(b_img==0)\n",
    "    temp=[]\n",
    "    if (black==0):\n",
    "        temp.append(0)\n",
    "        return temp\n",
    "    temp.append(white/black )\n",
    "    return  temp  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_white_ratio_up(b_img):\n",
    "    baseline_i=baseline(b_img)\n",
    "    white=np.sum(b_img[0:baseline_i,:]==1)\n",
    "    black=np.sum(b_img[0:baseline_i,:]==0)\n",
    "    temp=[]\n",
    "    if (black==0):\n",
    "        temp.append(0)\n",
    "        return temp\n",
    "    temp.append(white/black )\n",
    "    return  temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def black_white_ratio_down(b_img):\n",
    "    baseline_i=baseline(b_img)\n",
    "    white=np.sum(b_img[baseline_i:,:]==1)\n",
    "    black=np.sum(b_img[baseline_i:,:]==0)\n",
    "    temp=[]\n",
    "    if (black==0):\n",
    "        temp.append(0)\n",
    "        return temp\n",
    "    temp.append(white/black )\n",
    "    return  temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_up(b_img,min_area):\n",
    "    baseline_i=baseline(b_img)\n",
    "    temp=[]\n",
    "    if baseline_i<=2:\n",
    "        temp.append(0)\n",
    "        return temp \n",
    "    y,_=count_contours_full(b_img,min_area)\n",
    "    x,_=count_contours_full(b_img[0:baseline_i,:],min_area)\n",
    "    \n",
    "    temp.append(x/y)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_down(b_img,min_area):\n",
    "    baseline_i=baseline(b_img)\n",
    "    temp=[]\n",
    "    if baseline_i>=b_img.shape[0]-5:\n",
    "        show_images([b_img],['nn'])\n",
    "        temp.append(0)\n",
    "        return temp \n",
    "    y,_=count_contours_full(b_img,min_area)\n",
    "    x,_=count_contours_full(b_img[baseline_i:,:],min_area)\n",
    "    \n",
    "    temp.append(x/y)\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_rect(img):\n",
    "    rotations=[]\n",
    "    contours = cv2.findContours(img.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = contours[0] \n",
    "    for j,contour in enumerate(contours):\n",
    "            box = cv2.minAreaRect(contour)\n",
    "            rotations.append(box[2])   \n",
    "    avg_angle=sum(rotations)/len(rotations) \n",
    "    return avg_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### takes binary image and returns skeleton, edge\n",
    "def processing_images(binary_img):\n",
    "    bw_img=binary_img\n",
    "    sobel_img = sobel(bw_img)\n",
    "    skeletonized_img=skeletonize(bw_img)\n",
    "    return sobel_img, skeletonized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### function that returns num of horizontal and vertical lines\n",
    "def HVSL(bw_image):\n",
    "    # hough line to detect lines in the photo\n",
    "    tested_angles = np.linspace(-np.pi, np.pi, 360)\n",
    "    h, theta, d = hough_line(bw_image, theta=tested_angles)\n",
    "    origin = np.array((0, bw_image.shape[1]))\n",
    "    # hough peaks to get those lines\n",
    "    angles=[]\n",
    "    for _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n",
    "        y0, y1 = (dist - origin * np.cos(angle)) / np.sin(angle)\n",
    "        angles.append(angle)\n",
    "    angles = [angle * 180 / np.pi for angle in angles]\n",
    "    return angles.count(90.0), angles.count(180.0), len(angles)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### 1st feature ##################\n",
    "def HVSL_features(edge):\n",
    "    vertical_lines, horizontal_lines, lines = HVSL(edge)\n",
    "    if(vertical_lines+horizontal_lines) == 0:\n",
    "        horizontal_lines=.0001\n",
    "    if(lines==0):\n",
    "        lines=.0001\n",
    "    freq_appearance_ratio = np.count_nonzero(edge)/lines\n",
    "    ratio_pixels_HVL = np.count_nonzero(edge)/freq_appearance_ratio\n",
    "    return freq_appearance_ratio, ratio_pixels_HVL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_height_feature(skeleton):\n",
    "    skeleton = img_as_ubyte(skeleton)\n",
    "    kernel_vertical_line=np.ones((3,1))\n",
    "    line = cv2.morphologyEx(skeleton, cv2.MORPH_OPEN, kernel_vertical_line)\n",
    "    line=line/255\n",
    "    num_of_verticle_lines, max_vertical_line_height, variance = count_contour(line,3*1)\n",
    "    _, start_height, end_height  = auto_crop(skeleton)\n",
    "    text_height = abs(start_height-end_height)\n",
    "    return text_height, num_of_verticle_lines, max_vertical_line_height, text_height/max_vertical_line_height , variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_thickness(edge_img, skeleton_img):\n",
    "    row = min (skeleton_img.shape[0], edge_img.shape[0])\n",
    "    col = min (skeleton_img.shape[1], edge_img.shape[1])\n",
    "    dist = []\n",
    "    left = 0\n",
    "    right = 0\n",
    "    l = False\n",
    "    r = False\n",
    "    for i in range (row-1):\n",
    "        for j in range (col-1):\n",
    "            l = False\n",
    "            r = False\n",
    "            if skeleton_img[i][j] == 1:\n",
    "                max1 = max(j-10,0)\n",
    "                min2 = min(j+10,col)\n",
    "                for k in range(j,max1,-1):\n",
    "                    if edge_img[i][k] > 0:\n",
    "                        left = k\n",
    "                        l = True\n",
    "                        break\n",
    "                for g in range(j,min2):\n",
    "                    if edge_img[i][g] > 0:\n",
    "                        right = g\n",
    "                        r = True\n",
    "                        break\n",
    "                if l == False:\n",
    "                    left = j\n",
    "                if r == False:\n",
    "                    right = j\n",
    "                dist.append(abs(right-left))\n",
    "    h = np.histogram(dist)\n",
    "    return h[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### function that extracts hog features from dataset ###################\n",
    "def process_LVL_HVSL(x):\n",
    "    featuresLVL =[]\n",
    "    featuresHVSL=[]\n",
    "    HPP_features=[]\n",
    "    featuresToE=[]\n",
    "    featuresToS=[]\n",
    "    featuresThickness=[]\n",
    "    HOG=[]\n",
    "    black_white=[]\n",
    "    black_white_up=[]\n",
    "    black_white_down=[]\n",
    "    d_up=[]\n",
    "    d_down=[]\n",
    "    m_rect=[]\n",
    "    for _, path in enumerate(x):\n",
    "        img = io.imread(path, 1)\n",
    "        HOG.append(extract_hog_features(img))\n",
    "        bw_img=local_binarize(img)\n",
    "        edge, skeleton = processing_images(bw_img)\n",
    "        featuresLVL.append(text_height_feature(skeleton))\n",
    "        featuresHVSL.append(HVSL_features(edge))\n",
    "        baseline_i=baseline(bw_img)\n",
    "        if np.sum(bw_img[baseline_i])>=(bw_img.shape[1]-2):\n",
    "            bw_img= 1-bw_img\n",
    "        HPP_features.append(HPP(bw_img))\n",
    "        featuresThickness.append(text_thickness(edge, skeleton))\n",
    "        skeleton = img_as_ubyte(skeleton)\n",
    "        edge = img_as_ubyte(edge)\n",
    "        featuresToS.append(extract_hog_features(skeleton))\n",
    "        featuresToE.append(extract_hog_features(edge))\n",
    "        black_white.append(black_white_ratio(bw_img))\n",
    "        black_white_up.append(black_white_ratio_up(bw_img))\n",
    "        black_white_down.append(black_white_ratio_down(bw_img))\n",
    "        d_up.append(density_up(bw_img,3))\n",
    "        d_down.append(density_down(bw_img,3))\n",
    "        m_rect.append([min_rect(bw_img)])\n",
    "    return featuresLVL,featuresHVSL, HPP_features, featuresToS, featuresToE,featuresThickness, HOG,black_white,black_white_up,black_white_down,d_up,d_down,m_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################ demo test for LVL extraction from dataset using Decision Trees\n",
    "def main(x):\n",
    "    XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect = process_LVL_HVSL(x)\n",
    "    return XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11832/2224542135.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mXLVL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHVSL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPP_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesThickness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHOG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_rect\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    #Initializing the MLPClassifier\n",
    "\n",
    "\n",
    "    # model =   # x=[]\n",
    "x = glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\1\\\\*\")\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\2\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\3\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\4\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\5\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\6\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\7\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\8\\\\*\"))\n",
    "x.extend(glob.glob(\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\9\\\\*\"))\n",
    "_, y = load_images_from_folder([\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\1\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\2\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\3\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\4\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\5\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\6\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\7\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\8\",\"C:\\\\Users\\\\ok\\\\Downloads\\\\ACDB\\\\ACdata_base\\\\9\"])\n",
    "\n",
    "\n",
    "XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect  = main(x)\n",
    "\n",
    "\n",
    "\n",
    "    # 1,6,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (model):  97.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probabiliy = None\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(XLVL, y, test_size=0.1, random_state=1)\n",
    "\n",
    "XLVL_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy = XLVL_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(XHVSL, y, test_size=0.1, random_state=1)\n",
    "\n",
    "XHVSL_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += XHVSL_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(HPP_features, y, test_size=0.1, random_state=1)\n",
    "\n",
    "HPP_features_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += HPP_features_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(featuresToS, y, test_size=0.1, random_state=1)\n",
    "\n",
    "featuresToS_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += featuresToS_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(featuresToE, y, test_size=0.1, random_state=1)\n",
    "\n",
    "featuresToE_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += featuresToE_classifier.predict_proba(X_test)\n",
    "\n",
    "#(XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(featuresThickness, y, test_size=0.1, random_state=1)\n",
    "\n",
    "featuresThickness_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += featuresThickness_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(HOG, y, test_size=0.1, random_state=1)\n",
    "\n",
    "HOG_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += HOG_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(bw, y, test_size=0.1, random_state=1)\n",
    "\n",
    "bw_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += bw_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(bw_up, y, test_size=0.1, random_state=1)\n",
    "\n",
    "bw_up_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += bw_up_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(bw_down, y, test_size=0.1, random_state=1)\n",
    "\n",
    "bw_down_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += bw_down_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(m_rect, y, test_size=0.1, random_state=1)\n",
    "\n",
    "m_rect_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += m_rect_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(d_up, y, test_size=0.1, random_state=1)\n",
    "\n",
    "d_up_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += d_up_classifier.predict_proba(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(d_down, y, test_size=0.1, random_state=1)\n",
    "\n",
    "d_down_classifier = svm.SVC(kernel='rbf', degree=3, C=5, probability=True).fit(X_train, y_train)\n",
    "probabiliy += d_down_classifier.predict_proba(X_test)\n",
    "probabiliy_test = np.argmax(probabiliy, axis=1)+1\n",
    "clf_accuracy = accuracy_score(y_test, probabiliy_test)\n",
    "print('Accuracy (model): ', \"%.2f\" % (clf_accuracy*100))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predection(folder):\n",
    "    f = open(\"results.txt\", \"w\")\n",
    "    probabiliy = None\n",
    "    for filename in os.listdir(folder):\n",
    "        XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect = process_LVL_HVSL([os.path.join(folder,filename)])\n",
    "        probabiliy += XLVL_classifier.predict_proba(XLVL)+XHVSL_classifier.predict_proba(XHVSL)+HPP_features_classifier.predict_proba(HPP_features)+featuresToS_classifier.predict_proba(featuresToS)+featuresToE_classifier.predict_proba(featuresToE)\n",
    "        probabiliy += featuresThickness_classifier.predict_proba(featuresThickness)+HOG_classifier.predict_proba(HOG)+bw_classifier.predict_proba(bw)+bw_up_classifier.predict_proba(bw_up)+bw_down_classifier.predict_proba(bw_down)\n",
    "        probabiliy += d_up_classifier.predict_proba(d_up)+d_down_classifier.predict_proba(d_down)+m_rect_classifier.predict_proba(m_rect)\n",
    "        f.write(str(np.argmax(probabiliy)[0]+1)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_LVL_HVSL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11832/303027220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\ok\\\\Downloads\\\\Project Submission\\\\Project Submission\\\\test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11832/2946975252.py\u001b[0m in \u001b[0;36mpredection\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprobabiliy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mXLVL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHVSL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPP_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesThickness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHOG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_rect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_LVL_HVSL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprobabiliy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXLVL_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXLVL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mXHVSL_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXHVSL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mHPP_features_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHPP_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfeaturesToS_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesToS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfeaturesToE_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesToE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprobabiliy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfeaturesThickness_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesThickness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mHOG_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHOG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbw_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbw_up_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw_up\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbw_down_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw_down\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_LVL_HVSL' is not defined"
     ]
    }
   ],
   "source": [
    "predection(\"C:\\\\Users\\\\ok\\\\Downloads\\\\Project Submission\\\\Project Submission\\\\test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

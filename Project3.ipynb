{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_ubyte\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import cv2\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import  flood_fill\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.feature import hog\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import rectangle\n",
    "import math\n",
    "from matplotlib.pyplot import bar\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import hough_line, hough_line_peaks, rotate\n",
    "\n",
    "import numpy as np\n",
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_crop(binary_image: np.ndarray) -> np.ndarray:\n",
    "    all_points = cv2.findNonZero(binary_image)\n",
    "    x, y, w, h = cv2.boundingRect(all_points)\n",
    "    height, width = binary_image.shape\n",
    "    border = 0\n",
    "    left = max(0, x - border)\n",
    "    right = min(width, x + w + border)\n",
    "    top = max(0, y - border)\n",
    "    bottom = min(height, y + h + border)\n",
    "    return binary_image[top:bottom, left:right], top, bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## return hog features from input image #########\n",
    "def extract_hog_features(img):\n",
    "    img = cv2.resize(img, (110, 200))\n",
    "    fd = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), visualize=False)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### return hog features from arr of images ###########\n",
    "def feature_extraction_hog(images):\n",
    "    feature = []\n",
    "    for img in images:\n",
    "        img = img.astype('uint8')\n",
    "        img = cv2.resize(img, (64, 64), cv2.INTER_AREA)\n",
    "        featuresHog = extract_hog_features(img.astype('uint8'))\n",
    "        feature.append(featuresHog)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## binarize image ##########\n",
    "def local_binarize(img, block_size = 35, offset_val = 10):\n",
    "    img=img_as_ubyte(img)\n",
    "    b_img= img < threshold_otsu(img)\n",
    "    return b_img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### function that returns the row of the text basline #######\n",
    "def baseline(img):\n",
    "    horizontal_projection = np.sum(img, axis=1)\n",
    "    return np.argmax(horizontal_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### function that takes the folder of the dataset then returns array of images and their labels ########\n",
    "def load_images_from_folder(folders):\n",
    "    images = []\n",
    "    y=[]\n",
    "    i = 1\n",
    "    for folder in folders:   \n",
    "        for filename in os.listdir(folder):\n",
    "            img = io.imread(os.path.join(folder,filename), 1)\n",
    "            if img is not None:\n",
    "                b_img=local_binarize(img)\n",
    "                baseline_i=baseline(b_img)\n",
    "                if np.sum(b_img[baseline_i])>=(b_img.shape[1]-2):\n",
    "                    b_img= 1-b_img\n",
    "                images.append(b_img)\n",
    "                \n",
    "                y.append(i)\n",
    "        i+=1\n",
    "    return images, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### function that extracts hog features from dataset ###################\n",
    "def process_Hog():    \n",
    "    features = feature_extraction_hog(images)\n",
    "    return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "x=[]\n",
    "x = glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\1\\\\*\")\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\2\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\3\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\4\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\5\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\6\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\7\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\8\\\\*\"))\n",
    "x.extend(glob.glob(\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\9\\\\*\"))\n",
    "# images, y = load_images_from_folder([\"F:\\Downloads\\ACdata_base\\\\5\",\"F:\\Downloads\\ACdata_base\\\\8\",\"F:\\Downloads\\ACdata_base\\\\9\", \"F:\\Downloads\\ACdata_base\\\\3\"])\n",
    "images, y = load_images_from_folder([\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\1\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\2\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\3\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\4\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\5\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\6\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\7\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\8\",\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\9\"])\n",
    "# images, y = load_images_from_folder([\"F:\\\\GitHub\\\\Arabic-Calligraphy-Classification\\ACdata_base\\\\1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Horizontal profile projection ###########\n",
    "def HPP(img):\n",
    "    horizontal_projection = np.sum(img, axis=1)\n",
    "    # horizontal_projection=peak_local_max(horizontal_projection, min_distance=3).size\n",
    "    return np.histogram(horizontal_projection, bins=10)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contour(img,minArea):\n",
    "    find_contour_img=find_contours(img, 0.8)\n",
    "    i=0\n",
    "    vertical_lines_height = []\n",
    "    max_height = 0.0001\n",
    "    variance=0\n",
    "    for box in find_contour_img:  \n",
    "        Xmin=min(box[:,1])\n",
    "        Xmax=max(box[:,1])\n",
    "        Ymin=min(box[:,0])\n",
    "        Ymax=max(box[:,0])\n",
    "        if (Ymax-Ymin)*(Xmax-Xmin)>minArea:\n",
    "            vertical_lines_height.append(Ymax-Ymin)\n",
    "            i+=1\n",
    "    if vertical_lines_height != []:\n",
    "        max_height=max(vertical_lines_height)\n",
    "        variance=np.var(vertical_lines_height)\n",
    "    return i, max_height, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contours_full(img,minArea):\n",
    "    img=img.astype('uint8')\n",
    "    find_contour_img=find_contours(img,0.8)\n",
    "    final= np.zeros(img.shape)\n",
    "    i=0\n",
    "    a=[]\n",
    "    for box in find_contour_img:  \n",
    "        Xmin=min(box[:,1])\n",
    "        Xmax=max(box[:,1])\n",
    "        Ymin=min(box[:,0])\n",
    "        Ymax=max(box[:,0])\n",
    "        if (Ymax-Ymin)*(Xmax-Xmin)>minArea:\n",
    "            rr, cc = rectangle(start = (math.ceil(0),math.ceil(Xmin)), end = (math.ceil(img.shape[0]),math.ceil(Xmax)), shape=img.shape)\n",
    "            final[rr,cc]=1\n",
    "            i+=1\n",
    "            a.append([[Ymax,Xmax],[Ymax,Xmin],[Ymin,Xmin],[Ymin,Xmax]])\n",
    "\n",
    "    return i, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_white_ratio(b_img):\n",
    "    white=np.sum(b_img==1)\n",
    "    black=np.sum(b_img==0)\n",
    "    temp=[]\n",
    "    if (black==0):\n",
    "        temp.append(0)\n",
    "        return temp\n",
    "    temp.append(white/black )\n",
    "    return  temp  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_white_ratio_up(b_img):\n",
    "    baseline_i=baseline(b_img)\n",
    "    white=np.sum(b_img[0:baseline_i,:]==1)\n",
    "    black=np.sum(b_img[0:baseline_i,:]==0)\n",
    "    temp=[]\n",
    "    if (black==0):\n",
    "        temp.append(0)\n",
    "        return temp\n",
    "    temp.append(white/black )\n",
    "    return  temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def black_white_ratio_down(b_img):\n",
    "    baseline_i=baseline(b_img)\n",
    "    white=np.sum(b_img[baseline_i:,:]==1)\n",
    "    black=np.sum(b_img[baseline_i:,:]==0)\n",
    "    temp=[]\n",
    "    if (black==0):\n",
    "        temp.append(0)\n",
    "        return temp\n",
    "    temp.append(white/black )\n",
    "    return  temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_up(b_img,min_area):\n",
    "    baseline_i=baseline(b_img)\n",
    "    temp=[]\n",
    "    if baseline_i<=2:\n",
    "        temp.append(0)\n",
    "        return temp \n",
    "    y,_=count_contours_full(b_img,min_area)\n",
    "    x,_=count_contours_full(b_img[0:baseline_i,:],min_area)\n",
    "    \n",
    "    temp.append(x/y)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_down(b_img,min_area):\n",
    "    baseline_i=baseline(b_img)\n",
    "    temp=[]\n",
    "    if baseline_i>=b_img.shape[0]-5:\n",
    "        show_images([b_img],['nn'])\n",
    "        temp.append(0)\n",
    "        return temp \n",
    "    y,_=count_contours_full(b_img,min_area)\n",
    "    x,_=count_contours_full(b_img[baseline_i:,:],min_area)\n",
    "    \n",
    "    temp.append(x/y)\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_rect(img):\n",
    "    rotations=[]\n",
    "    contours = cv2.findContours(img.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = contours[0] \n",
    "    for j,contour in enumerate(contours):\n",
    "            box = cv2.minAreaRect(contour)\n",
    "            rotations.append(box[2])   \n",
    "    avg_angle=sum(rotations)/len(rotations) \n",
    "    return avg_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### takes binary image and returns skeleton, edge\n",
    "def processing_images(binary_img):\n",
    "    bw_img=binary_img\n",
    "    sobel_img = sobel(bw_img)\n",
    "    skeletonized_img=skeletonize(bw_img)\n",
    "    return sobel_img, skeletonized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### function that returns num of horizontal and vertical lines\n",
    "def HVSL(bw_image):\n",
    "    # hough line to detect lines in the photo\n",
    "    tested_angles = np.linspace(-np.pi, np.pi, 360)\n",
    "    h, theta, d = hough_line(bw_image, theta=tested_angles)\n",
    "    origin = np.array((0, bw_image.shape[1]))\n",
    "    # hough peaks to get those lines\n",
    "    angles=[]\n",
    "    for _, angle, dist in zip(*hough_line_peaks(h, theta, d)):\n",
    "        y0, y1 = (dist - origin * np.cos(angle)) / np.sin(angle)\n",
    "        angles.append(angle)\n",
    "    angles = [angle * 180 / np.pi for angle in angles]\n",
    "    return angles.count(90.0), angles.count(180.0), len(angles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### 1st feature ##################\n",
    "def HVSL_features(edge):\n",
    "    vertical_lines, horizontal_lines, lines = HVSL(edge)\n",
    "    if(vertical_lines+horizontal_lines) == 0:\n",
    "        horizontal_lines=.0001\n",
    "    if(lines==0):\n",
    "        lines=.0001\n",
    "    freq_appearance_ratio = np.count_nonzero(edge)/lines\n",
    "    ratio_pixels_HVL = np.count_nonzero(edge)/freq_appearance_ratio\n",
    "    return freq_appearance_ratio, ratio_pixels_HVL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_height_feature(skeleton):\n",
    "    skeleton = img_as_ubyte(skeleton)\n",
    "    kernel_vertical_line=np.ones((3,1))\n",
    "    line = cv2.morphologyEx(skeleton, cv2.MORPH_OPEN, kernel_vertical_line)\n",
    "    line=line/255\n",
    "    num_of_verticle_lines, max_vertical_line_height, variance = count_contour(line,3*1)\n",
    "    _, start_height, end_height  = auto_crop(skeleton)\n",
    "    text_height = abs(start_height-end_height)\n",
    "    return text_height, num_of_verticle_lines, max_vertical_line_height, text_height/max_vertical_line_height , variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_thickness(edge_img, skeleton_img):\n",
    "    row = min (skeleton_img.shape[0], edge_img.shape[0])\n",
    "    col = min (skeleton_img.shape[1], edge_img.shape[1])\n",
    "    dist = []\n",
    "    left = 0\n",
    "    right = 0\n",
    "    l = False\n",
    "    r = False\n",
    "    for i in range (row-1):\n",
    "        for j in range (col-1):\n",
    "            l = False\n",
    "            r = False\n",
    "            if skeleton_img[i][j] == 1:\n",
    "                max1 = max(j-10,0)\n",
    "                min2 = min(j+10,col)\n",
    "                for k in range(j,max1,-1):\n",
    "                    if edge_img[i][k] > 0:\n",
    "                        left = k\n",
    "                        l = True\n",
    "                        break\n",
    "                for g in range(j,min2):\n",
    "                    if edge_img[i][g] > 0:\n",
    "                        right = g\n",
    "                        r = True\n",
    "                        break\n",
    "                if l == False:\n",
    "                    left = j\n",
    "                if r == False:\n",
    "                    right = j\n",
    "                dist.append(abs(right-left))\n",
    "    h = np.histogram(dist)\n",
    "    return h[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### function that extracts hog features from dataset ###################\n",
    "def process_LVL_HVSL(x):\n",
    "    featuresLVL =[]\n",
    "    featuresHVSL=[]\n",
    "    HPP_features=[]\n",
    "    featuresToE=[]\n",
    "    featuresToS=[]\n",
    "    featuresThickness=[]\n",
    "    HOG=[]\n",
    "    black_white=[]\n",
    "    black_white_up=[]\n",
    "    black_white_down=[]\n",
    "    d_up=[]\n",
    "    d_down=[]\n",
    "    m_rect=[]\n",
    "    for index, path in enumerate(x):\n",
    "        img = io.imread(path, 1)\n",
    "        HOG.append(extract_hog_features(img))\n",
    "        bw_img=local_binarize(img)\n",
    "\n",
    "        edge, skeleton = processing_images(bw_img)\n",
    "        # show_images([edge],[\"edge\"])\n",
    "        featuresLVL.append(text_height_feature(skeleton))\n",
    "        featuresHVSL.append(HVSL_features(edge))\n",
    "        baseline_i=baseline(bw_img)\n",
    "        if np.sum(bw_img[baseline_i])>=(bw_img.shape[1]-2):\n",
    "            bw_img= 1-bw_img\n",
    "        HPP_features.append(HPP(bw_img))\n",
    "        featuresThickness.append(text_thickness(edge, skeleton))\n",
    "        skeleton = img_as_ubyte(skeleton)\n",
    "        edge = img_as_ubyte(edge)\n",
    "        featuresToS.append(extract_hog_features(skeleton))\n",
    "        featuresToE.append(extract_hog_features(edge))\n",
    "        black_white.append(black_white_ratio(bw_img))\n",
    "        black_white_up.append(black_white_ratio_up(bw_img))\n",
    "        black_white_down.append(black_white_ratio_down(bw_img))\n",
    "        d_up.append(density_up(bw_img,3))\n",
    "        d_down.append(density_down(bw_img,3))\n",
    "        m_rect.append([min_rect(bw_img)])\n",
    "    bw=np.histogram(black_white)\n",
    "    bw_up=np.histogram(black_white_up)\n",
    "    bw_down=np.histogram(black_white_down)\n",
    "    # return \n",
    "    return featuresLVL,featuresHVSL, HPP_features, featuresToS, featuresToE,featuresThickness, HOG,black_white,black_white_up,black_white_down,d_up,d_down,m_rect,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################ demo test for LVL extraction from dataset using Decision Trees\n",
    "def main():\n",
    "    XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect,y = process_LVL_HVSL(x)\n",
    "\n",
    "    ################ demo test for hog extraction from dataset using support vector machine\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(HOG, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "    poly = svm.SVC(kernel='rbf', degree=3, C=5).fit(X_train, y_train)\n",
    "    poly_pred = poly.predict(X_test)\n",
    "    poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "    poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "    print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "    print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    "    X = np.hstack((XLVL, XHVSL, HPP_features, featuresToE, featuresThickness,HOG,bw,bw_up,d_up, d_down,m_rect))\n",
    "\n",
    "    voting_clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "    eclf1 = voting_clf.fit(X_train, y_train)\n",
    "    return eclf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6736/4242256252.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6736/1143189779.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m################ demo test for LVL extraction from dataset using Decision Trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mXLVL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHVSL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPP_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesThickness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHOG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_rect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_LVL_HVSL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m################ demo test for hog extraction from dataset using support vector machine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6736/1245038749.py\u001b[0m in \u001b[0;36mprocess_LVL_HVSL\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0medge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskeleton\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessing_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# show_images([edge],[\"edge\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mfeaturesLVL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_height_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskeleton\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mfeaturesHVSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHVSL_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mbaseline_i\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbw_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6736/727866680.py\u001b[0m in \u001b[0;36mtext_height_feature\u001b[1;34m(skeleton)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mskeleton\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_as_ubyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskeleton\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkernel_vertical_line\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphologyEx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskeleton\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMORPH_OPEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_vertical_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnum_of_verticle_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_vertical_line_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_contour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eclf1 = main()\n",
    "def predection(folder):\n",
    "    f = open(\"results.txt\", \"a\")\n",
    "    for filename in os.listdir(folder):\n",
    "        print(filename)\n",
    "        XLVL, XHVSL, HPP_features, featuresToS, featuresToE, featuresThickness, HOG,bw,bw_up,bw_down,d_up,d_down,m_rect,y = process_LVL_HVSL([filename])\n",
    "        features = np.hstack((XLVL, XHVSL, HPP_features, featuresToE, featuresThickness,HOG,bw,bw_up,d_up, d_down,m_rect))\n",
    "        poly_pred = eclf1.predict(features)\n",
    "        f.write(poly_pred)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "process_LVL_HVSL() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6736/303027220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\ok\\\\Downloads\\\\Project Submission\\\\Project Submission\\\\test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6736/3273298494.py\u001b[0m in \u001b[0;36mpredection\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mXLVL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHVSL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPP_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesThickness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHOG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_rect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_LVL_HVSL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXLVL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXHVSL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPP_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesToE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesThickness\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHOG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_down\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_rect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpoly_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meclf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: process_LVL_HVSL() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "predection(\"C:\\\\Users\\\\ok\\\\Downloads\\\\Project Submission\\\\Project Submission\\\\test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
